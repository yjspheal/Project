{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4bf024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 2 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uj200\\AppData\\Local\\Temp/ipykernel_43424/1243582925.py:18: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(\".\\chromedriver.exe\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YT_data 폴더가 이미 존재합니다. 해당 폴더에 데이터를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uj200\\AppData\\Local\\Temp/ipykernel_43424/1243582925.py:33: DeprecationWarning: find_element_by_tag_name is deprecated. Please use find_element(by=By.TAG_NAME, value=name) instead\n",
      "  body = browser.find_element_by_tag_name('body')\n",
      "C:\\Users\\uj200\\AppData\\Local\\Temp/ipykernel_43424/1243582925.py:62: DeprecationWarning: find_element_by_tag_name is deprecated. Please use find_element(by=By.TAG_NAME, value=name) instead\n",
      "  body = browser.find_element_by_tag_name('body')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 3 YT_data 폴더가 이미 존재합니다. 해당 폴더에 데이터를 저장합니다.\n",
      "Message: chrome not reachable\n",
      "  (Session info: chrome=99.0.4844.84)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00699943+2595139]\n",
      "\tOrdinal0 [0x0062C9F1+2148849]\n",
      "\tOrdinal0 [0x005243F0+1065968]\n",
      "\tOrdinal0 [0x005187C2+1017794]\n",
      "\tOrdinal0 [0x00518FF8+1019896]\n",
      "\tOrdinal0 [0x0051A892+1026194]\n",
      "\tOrdinal0 [0x00514219+999961]\n",
      "\tOrdinal0 [0x00525860+1071200]\n",
      "\tOrdinal0 [0x0057B2D2+1422034]\n",
      "\tOrdinal0 [0x0056B806+1357830]\n",
      "\tOrdinal0 [0x00546086+1204358]\n",
      "\tOrdinal0 [0x00546F96+1208214]\n",
      "\tGetHandleVerifier [0x0083B232+1658114]\n",
      "\tGetHandleVerifier [0x008F312C+2411516]\n",
      "\tGetHandleVerifier [0x0072F261+560433]\n",
      "\tGetHandleVerifier [0x0072E366+556598]\n",
      "\tOrdinal0 [0x0063286B+2173035]\n",
      "\tOrdinal0 [0x006375F8+2192888]\n",
      "\tOrdinal0 [0x006376E5+2193125]\n",
      "\tOrdinal0 [0x006411FC+2232828]\n",
      "\tBaseThreadInitThunk [0x768CFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77527A7E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77527A4E+238]\n",
      "\n",
      "파일 에러 발생.필요한 데이터는 아래와 같습니다.\n",
      "원본_올리브영랭크100위화장품정보_상세페이지.xlsx\n",
      "원본_트렌드 지수(아이템 스카우트)/트렌드지수 크롤링(브랜드).xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import openpyxl as xl\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_youtube_data(brand, goods):\n",
    "    search_goods = f'{brand} {goods}'\n",
    "    \n",
    "    # 드라이버 주소 설정\n",
    "    browser = webdriver.Chrome(\".\\chromedriver.exe\") \n",
    "    browser.implicitly_wait(1)   \n",
    "    \n",
    "    # 크롤링 데이터를 저장할 경로 및 엑셀 파일 이름 지정\n",
    "    try:      # 데이터 100개를 저장할 폴더 생성('YT_data') 및 이미 존재하는 폴더일 시 예외처리\n",
    "        os.mkdir(\"new_YT_data\") \n",
    "    except FileExistsError:\n",
    "        print('YT_data 폴더가 이미 존재합니다. 해당 폴더에 데이터를 저장합니다.')\n",
    "    result = pd.ExcelWriter(\"./new_YT_data/new_\" + search_goods + '.xlsx', engine='openpyxl')\n",
    "    \n",
    "    # 유튜브 검색을 위한 url을 만들어 열고 body 추출\n",
    "    yt_url = \"https://www.youtube.com\"\n",
    "    search_goods = search_goods.replace(' ','+')  # url용으로 제품명 사이 공백 +로 대체\n",
    "    target_url  = yt_url + \"/results?search_query=\"+search_goods + \"+리뷰\" + \"&sp=CAI%253D\"      # 업로드날짜 필터링\n",
    "    browser.get(target_url)\n",
    "    body = browser.find_element_by_tag_name('body')\n",
    "    \n",
    "    # 전체 데이터를 추출하기 위하여 페에지를 쭉 내림. 필터링이 되기 때문에 50번이면 충분\n",
    "    for pg_down in range(50):  # 페이지 다운 수\n",
    "        body.send_keys(Keys.PAGE_DOWN)\n",
    "        browser.implicitly_wait(1)\n",
    "    \n",
    "    # 해당 페이지 html 소스를 beautifulsoup을 이용하여 html 에 저장\n",
    "    html0 = browser.page_source\n",
    "    html = BeautifulSoup(html0,'html.parser')\n",
    "    \n",
    "    # 검색 결과창에서 각 비디오 하나씩을 나누어줌\n",
    "    video_datas = html.find_all('ytd-video-renderer',{'class':'style-scope ytd-item-section-renderer'})\n",
    "\n",
    "    \n",
    "    #데이터 저장을 위한 기본 데이터프레임 생성\n",
    "    dataframe = pd.DataFrame({'title':[], 'youtube_url':[], 'subscribers':[], 'views':[], \"nice\":[], \"reply\":[]})\n",
    "    \n",
    "    # 비디오 url을 저장할 리스트 생성(해당 url은 youtube.com/ 이후에 들어가는 부분)\n",
    "    video_url_list = []\n",
    "    for i in range(len(video_datas)):\n",
    "        # 검색 결과 창에서 얻을 수 있는 데이터인 동영상 제목(title)과 동영상 링크 추출\n",
    "        title = video_datas[i].find('a',{'id':'video-title'}).get_text() \n",
    "        url = yt_url + video_datas[i].find('a',{'id':'thumbnail'})['href'] \n",
    "        video_url_list.append(url)\n",
    "        \n",
    "        # 상세 동영상 페이지 접속 및 바디 추출\n",
    "        cur_url = video_url_list[i]\n",
    "        browser.get(cur_url); time.sleep(5)\n",
    "        body = browser.find_element_by_tag_name('body')\n",
    "        \n",
    "        # 댓글 수를 보려면 페이지를 동영상 목록까지 내린 후 기다려야 나오므로 페이지를 내리고 기다리는 과정을 반복\n",
    "        for pg_down in range(10):\n",
    "            body.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # 현재 동영상 페이지 소스코드를 beautifulsoup 이용하여 html에 저장\n",
    "        html0 = browser.page_source\n",
    "        html = BeautifulSoup(html0,'html.parser')\n",
    "        \n",
    "        # 조회수, 좋아요수, 댓글수는 영상마다 감춰두거나 막아놓는 경우가 있어 에러가 발생하여 예외처리\n",
    "        try:\n",
    "            \n",
    "            # 조회수를 크롤링하고, nnn,nnn회 라는 결과를 얻게 되어 replace로 정리하여 정수로 저장\n",
    "            view_count = html.find('span',{'class':'view-count style-scope ytd-video-view-count-renderer'}).get_text().split()[1]\n",
    "            view_count = view_count.replace(\",\", \"\"); view_count = view_count.replace(\"회\", \"\"); view_count = int(view_count)\n",
    "            \n",
    "            # 채널명(유튜버)를 크롤링하여 youtuber에 저장\n",
    "            channel = html.find(\"div\", {'class' : 'style-scope ytd-channel-name'})     # 채널명을 나타내는 태그와 class는 여러 곳에서 쓰이므로, class style-scope ytd-channel-name를 이용하여 chammel에 저장 후 chammel에서 해당 형태를 다시 찾아냄\n",
    "            youtuber = channel.find(\"a\", {\"class\" : 'yt-simple-endpoint style-scope yt-formatted-string'}).get_text()\n",
    "            \n",
    "            # 데이터 정제를 위하여 조회수가 500 미만이거나 채널 명에 브랜드가 포함되는 경우(즉 브랜드 자체 광고 동영상인 경우) 해당 동영상은 넘기고 다음 '동영상'으로 넘어감\n",
    "            if view_count < 500 or youtuber.find(brand) != -1:\n",
    "                continue       \n",
    "            \n",
    "            # 업로드 일자를 크롤링하여 만약 현재(코드를 돌리는 시점)부터 6개월 이전에 만들어진 영상이라면 break하고 다음 '제품'으로 넘어감(업로드 일자를 기준으로 정렬된 동영상들이기 때문에 이 이후는 볼 필요가 없으므로 break)\n",
    "            upload_date = html.find_all(\"yt-formatted-string\", {\"class\" : \"style-scope ytd-video-primary-info-renderer\"})[1].get_text()\n",
    "            daybefore = dt.datetime.today() - dt.timedelta(days = 180) \n",
    "            upload_date = upload_date[:len(upload_date)-1]\n",
    "            ul_year, ul_month, ul_day = map(int,upload_date.split(\".\"))\n",
    "            if daybefore > dt.datetime(ul_year, ul_month, ul_day):   # 업로드 6개월 이전이라면 break\n",
    "                break\n",
    "            \n",
    "            # 좋아요 수 크롤링, 좋아요 수는 싫어요 수와 같은 태그 및 class를 가지고 그중 첫번째에 해당함\n",
    "            nice = html.find_all(\"yt-formatted-string\",{\"class\" : \"style-scope ytd-toggle-button-renderer style-text\"})[0]\n",
    "            # get_text를 하면 'n.n만개'처럼 축약된 형태가 반환되고, 태그 속 aria-label에 상세한 좋아요 수가 들어있어 이를 반환\n",
    "            nice_count = nice[\"aria-label\"].split()[-1]     # '좋아요 nnnnn개'의 형태가 반환되므로 split함\n",
    "        \n",
    "            # 좋아요 수가 100개 이하인 경우 콤마가 포함되지 않는 숫자가 반환되어 replace가 에러날 수 있으므로 문자로 변환하였다가 정제 후 정수로 다시 변환하여 저장\n",
    "            nice_count = str(nice_count)\n",
    "            nice_count = nice_count.replace(\",\", \"\"); nice_count = nice_count.replace(\"개\", \"\")\n",
    "            nice_count = int(nice_count)\n",
    "            \n",
    "            # 구독자 수를 아래의 태그와 id를 이용하여 크롤링\n",
    "            subscribers = html.find(\"yt-formatted-string\",{\"id\" : \"owner-sub-count\"}).get_text().split()[1]\n",
    "            \n",
    "            # 댓글 수 크롤링. 댓글 수를 나타내는 태그와 class는 아래의 답글 수 등에서도 계속 사용되기 때문에 info에 댓글 수가 있는 comments-header를 추출하여 저장 후 사용\n",
    "            info = html.find('h2', {'class' : 'style-scope ytd-comments-header-renderer'})\n",
    "            reply_count = info.find_all('span', {'class' : 'style-scope yt-formatted-string'})[1].get_text()    #info에는 댓글/nnn/개 의 소스가 저장되어있어 2번째 인자 추출하여 get_text\n",
    "            reply_count = int(reply_count)    # nnn의 형태이나 문자이므로 정수로 변환하여 저장\n",
    "            \n",
    "            # 각 데이터를 합쳐 데이터프레임을 만들고 insert_data에 저장\n",
    "            insert_data = pd.DataFrame({'title':[title], 'youtube_url':[url], 'subscribers':[subscribers], 'views':[view_count], \"nice\":[nice_count], \"reply\":[reply_count]})\n",
    "            print(\"추출 Yes\\n\")  # 코드 실행 중 가시화를 위한 코드\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue \n",
    "         \n",
    "        # insert_data를 위에 만들었던 dataframe에 합쳐넣고, 이를 엑셀로 변환하여 저장\n",
    "        dataframe = dataframe.append(insert_data)\n",
    "        dataframe.to_excel(result, index = False)\n",
    "        result.save()\n",
    "    \n",
    "    # dataframe에 들어가는 데이터가 없는 경우, 즉 최근 6개월 이내에 올라온 조회수 500 이상의 리뷰 동영상이 없는 경우 엑셀이 제대로 저장되지 않으므로 데이터프레임을 하나 만들어서 저장\n",
    "    if dataframe.shape[0] == 0:\n",
    "        dataframe = pd.DataFrame({'title': [0], 'youtube_url': [0], 'subscribers':[0], 'views':[0], \"nice\":[0], \"reply\":[0]})   # 조회수가 0인 영상을 존재하지 않기 때문에 시각화 과정에서 이를 이용하여 실제 데이터와 분리\n",
    "        dataframe.to_excel(result, index = False)\n",
    "        result.save()\n",
    "    \n",
    "\n",
    "def youtube_crawling():\n",
    "    # 제품 명과 브랜드 이름이 들어있는 엑셀 파일을 불러와 활성화\n",
    "    raw_data = xl.load_workbook('원본_올리브영랭크100위화장품정보_상세페이지.xlsx')\n",
    "    raw_data = raw_data.active\n",
    "    \n",
    "    # C, D열에서 각각 제품 명과 브랜드 이름(이 들어있는 셀 리스트)를 slice하여 저장\n",
    "    goods_name = raw_data[\"D\"][1::]\n",
    "    brand_name = raw_data[\"E\"][1::]\n",
    "\n",
    "    # get_youtube_data 함수에 제품명과 브랜드를 넣어 실행\n",
    "    for i in range(1,100):\n",
    "        print(f'rank {i+1}',end = ' ')  # 코드 실행 중 가시화를 위한 코드\n",
    "        goods = goods_name[i].value   # _name[i] 자체는 셀이기 때문에 value로 셀의 내용을 추출하여 저장\n",
    "        brand = brand_name[i].value\n",
    "        get_youtube_data(brand, goods)\n",
    "        \n",
    "        \n",
    "# 기존 엑셀에 유튜브 데이터 추가하는 함수\n",
    "def excel_add_youtube():\n",
    "    # 기존 정보 들어있는 엑셀을 load하고 활성화시킴\n",
    "    raw_datad = xl.load_workbook('원본_올리브영랭크100위화장품정보_상세페이지.xlsx')\n",
    "    raw_data = raw_datad.active\n",
    "    \n",
    "    # 저장된 100개의 파일 불러오기 위한 브랜드, 제품명 가져오기\n",
    "    goods_name = raw_data[\"C\"][1::]; brand_name = raw_data[\"D\"][1::]\n",
    "    \n",
    "    # format을 맞추기 위해 첫번째 행에 들어갈 값들 지정\n",
    "    raw_data[\"Q1\"] = \"유튜브 컨텐츠 수\"; raw_data[\"R1\"] = \"총 조회수\"; raw_data[\"S1\"] = \"총 좋아요수\"; raw_data[\"T1\"] = \"총 댓글수\"\n",
    "    \n",
    "    # 100개의 파일을 불러와 기존 엑셀에 대하여 유튜브 컨텐츠 수, 총 조회수, 좋아요수, 댓글수 추출하여 저장\n",
    "    for i in range(100):\n",
    "        goods = goods_name[i].value\n",
    "        brand = brand_name[i].value\n",
    "\n",
    "        filed = xl.load_workbook('./원본_YT_data/'+f'{brand} {goods}'+\".xlsx\")  # 파일 이름으로 불러오므로 폴더 내에 데이터 순서에 무관하게 각 파일을 가져옴\n",
    "        file = filed.active\n",
    "        \n",
    "        # 각 파일 내에서 조회수, 좋아요수, 댓글 수 들어있는 열을 가져옴\n",
    "        views = file[\"D\"][1::]; nices = file[\"E\"][1::]; replies = file[\"F\"][1::] \n",
    "        \n",
    "        # 컨텐츠의 갯수는 파일의 행 갯수로 가져옴\n",
    "        contents_count = len(file[\"E\"][1::])\n",
    "        \n",
    "        # 각 셀의 value를 가져와 총 합계를 각각 구함\n",
    "        views_sum = 0; nices_sum = 0; replies_sum = 0\n",
    "        for v in views:\n",
    "            views_sum += v.value\n",
    "        \n",
    "        for n in nices:\n",
    "            nices_sum += n.value\n",
    "        \n",
    "        for r in replies:\n",
    "            replies_sum += r.value\n",
    "\n",
    "    # 구한 값들을 차례로 엑셀 칸에 집어넣음\n",
    "    raw_data[f\"Q{i+2}\"] = contents_count; raw_data[f\"R{i+2}\"] = views_sum; raw_data[f\"S{i+2}\"] = nices_sum; raw_data[f\"T{i+2}\"] = replies_sum\n",
    "    \n",
    "    # 만든 엑셀 데이터를 새로운 이름으로 저장\n",
    "    raw_datad.save('new_올리브영랭크100위화장품정보+유튜브데이터.xlsx')\n",
    "    \n",
    "def make_top20():\n",
    "    # 브랜드 top 20 리스트 생성(제품 top 20으로 브랜드 갯수는 더 적음)\n",
    "    tops = ['아누아','토리든','닥터지','라로슈포제','라운드랩','아벤느','더랩바이블랑두','디오디너리','셀리맥스','브링그린','에스트라']\n",
    "    \n",
    "    try: # 결과 저장할 엑셀 생성\n",
    "        top20d = pd.ExcelWriter('new_올리브영_진정_상위20.xlsx', engine='openpyxl')\n",
    "        dataframe = pd.DataFrame(['','',' ','','','','','',''])\n",
    "        dataframe.to_excel(top20d, index = False)\n",
    "        top20d.save()\n",
    "    except:\n",
    "        print(\"이미 파일이 존재합니다. 기존 파일에 덮어쓰기를 시작합니다.\")\n",
    "    \n",
    "    # 만든 파일 활성화\n",
    "    top20d = xl.load_workbook('new_올리브영_진정_상위20.xlsx')\n",
    "    top20 = top20d.active\n",
    "    \n",
    "    # 유튜브데이터까지 들어있는 엑셀을 load후 활성화\n",
    "    ol_yud = xl.load_workbook('원본_올리브영랭크100위화장품정보+유튜브데이터.xlsx')\n",
    "    ol_yu = ol_yud.active\n",
    "    \n",
    "    # 트렌드 지수 들어있는 엑셀 load 후 활성화\n",
    "    trendd = xl.load_workbook('./원본_트렌드지수 크롤링(브랜드).xlsx')\n",
    "    trend = trendd.active\n",
    "\n",
    "    # format을 맞추기 위해 첫번째 행에 들어갈 값들 지정\n",
    "    top20[\"B1\"] = \"브랜드\"; top20[\"C1\"] = \"평균가격\"; top20[\"D1\"] = \"평균리뷰개수\"; top20[\"E1\"] = \"평균별점\"\n",
    "    top20[\"F1\"] = \"평균진정\"; top20[\"G1\"] = \"평균좋아요/조회수\"; top20[\"H1\"] = \"유튜브 컨텐츠 수\"; top20[\"I1\"] = \"매출액\"; top20[\"J1\"] = \"유튜브 컨텐츠 수\"\n",
    "   \n",
    "    # ol_yu 속 데이터 추출\n",
    "    brands = ol_yu[\"D\"][1::]\n",
    "    prices = ol_yu[\"E\"][1::]\n",
    "    reviews = ol_yu[\"G\"][1::]\n",
    "    points = ol_yu[\"H\"][1::]\n",
    "    treats = ol_yu[\"O\"][1::]\n",
    "    contents = ol_yu[\"Q\"][1::]\n",
    "    views = ol_yu[\"R\"][1::]\n",
    "    nices = ol_yu[\"S\"][1::]\n",
    "    repls = ol_yu[\"T\"][1::]\n",
    "    \n",
    "    #trend 속 데이터 추출\n",
    "    sells = trend[\"D\"][1::]\n",
    "    searchs = trend[\"C\"][1::]\n",
    "    \n",
    "    # top 20 브랜드들에 대한 ol_yu속 데이터 처리\n",
    "    for k in range(len(tops)):\n",
    "        brand_count = 0\n",
    "        count_for_nocont = 0\n",
    "        count_for_noreview = 0\n",
    "        sumprice = 0\n",
    "        sumreview = 0\n",
    "        sumpoint = 0\n",
    "        sumtreat = 0\n",
    "        sumnpv = 0\n",
    "        sumcontents = 0\n",
    "        for i in range(100):\n",
    "            brand = brands[i].value\n",
    "            \n",
    "            if brand == tops[k]:\n",
    "                \n",
    "                brand_count += 1 ; count_for_nocont += 1; count_for_noreview += 1\n",
    "                price = prices[i].value.replace(\",\",\"\")\n",
    "                sumprice += int(price)\n",
    "                if reviews[i].value == -1:\n",
    "                    count_for_noreview -= 1\n",
    "                else:\n",
    "                    sumreview += reviews[i].value\n",
    "                    sumpoint += points[i].value\n",
    "                    sumtreat += treats[i].value\n",
    "                \n",
    "                if views[i].value == 0: # 조회수가 없는, 즉 검색 결과가 없는 경우 좋아요/조회수 와 컨텐츠 수에 포함되지 않도록 설정\n",
    "                    count_for_nocont -= 1\n",
    "                else:\n",
    "                    sumnpv += nices[i].value / views[i].value\n",
    "                    sumcontents += contents[i].value\n",
    "                \n",
    "                \n",
    "        # 구한 값들을 차례로 엑셀 칸에 집어넣음\n",
    "        \n",
    "        top20[f\"B{k+2}\"] = tops[k];\n",
    "        top20[f\"C{k+2}\"] = sumprice / brand_count;\n",
    "        \n",
    "        if count_for_noreview != 0:   #zero division error 방지\n",
    "            top20[f\"D{k+2}\"] = sumreview / count_for_noreview\n",
    "            top20[f\"E{k+2}\"] = sumpoint / count_for_noreview\n",
    "            top20[f\"F{k+2}\"] = sumtreat / count_for_noreview\n",
    "        else:\n",
    "            top20[f\"D{k+2}\"]=0; top20[f\"E{k+2}\"]=0; top20[f\"F{k+2}\"]=0\n",
    "        \n",
    "        if count_for_nocont != 0:  #zero division error 방지\n",
    "            top20[f\"G{k+2}\"] = sumnpv / count_for_nocont\n",
    "        else:\n",
    "            top20[f\"G{k+2}\"] = 0\n",
    "            \n",
    "        top20[f\"H{k+2}\"] = sumcontents\n",
    "        \n",
    "        \n",
    "    # top 20 브랜드들에 대한 trend속 데이터 처리\n",
    "    for k in range(len(tops)):\n",
    "        for i in range(len(sells)-2):\n",
    "            if trend[f\"B{i+2}\"].value == tops[k]:\n",
    "                top20[f'I{k+2}'] = sells[i].value\n",
    "                top20[f'J{k+2}'] = searchs[i].value\n",
    "                \n",
    "    #파일 저장\n",
    "    top20d.save('new_올리브영_진정_상위20.xlsx')\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    try :\n",
    "        youtube_crawling()\n",
    "        excel_add_youtube()\n",
    "        make_top20()\n",
    "    except Exception as e:\n",
    "        print('파일 에러 발생.필요한 데이터는 아래와 같습니다.')\n",
    "        print('원본_올리브영랭크100위화장품정보_상세페이지.xlsx')\n",
    "        print('트렌드지수 크롤링(브랜드).xlsx')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3306e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
